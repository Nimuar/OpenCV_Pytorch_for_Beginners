{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac994db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b3c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758568cd",
   "metadata": {},
   "source": [
    "# 1. Automatic Differentiation with torch.autograd\n",
    "\n",
    "Before proceeding autograd, will understand the basic terms:\n",
    "\n",
    "- Forward Propagation:\n",
    "    Computes the model's output by passing the input data through the network layers. It is often called Forward pass.\n",
    "\n",
    "- Backward Propagation:\n",
    "    Calculates the gradients of the loss with respect to the model's parameters using the chain rule, enabling parameter updates to minimize the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b19605",
   "metadata": {},
   "source": [
    "### 1.1. torch.autograd\n",
    "\n",
    "[Autograd docs](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#a-gentle-introduction-to-torch-autograd)\n",
    "\n",
    "- We create two tensors x and y with requires_grad=True, indicating that we want to compute gradients for these tensors.\n",
    "\n",
    "- We perform simple operations on x and y to obtain z.\n",
    "\n",
    "- Computing Gradients: We call z.backward() to compute the gradients of z with respect to x and y. The gradients are stored in the grad attribute of each tensor.\n",
    "\n",
    "In the following example:\n",
    "\n",
    "- The operation is z=x⋅y+y2\n",
    "\n",
    "- The partial derivative of z with respect to x is ∂z∂x=y\n",
    "\n",
    "- The partial derivative of z with respect to y is ∂z∂y=x+2y\n",
    "\n",
    "Given x=2.0 and y=3.0:\n",
    "\n",
    "- The gradient of z w.r.t. x is 3.0\n",
    "\n",
    "- The gradient of z w.r.t. y is 2.0+2⋅3.0=8.0\n",
    "\n",
    "Tensors that require gradients will have their operations tracked by PyTorch's autograd engine, enabling the computation of gradients during backpropagation.\n",
    "\n",
    "![Computation Graph Through Autograd](media/Autograd-Computation-Graph-2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3466fb1",
   "metadata": {},
   "source": [
    "The automatic differentiation provided by torch.autograd simplifies this process by handling the complex chain rule calculations needed for backpropagation through the entire network.\n",
    "\n",
    "For ∂z/∂x:  \n",
    "```math\n",
    "\\dfrac{∂z}{∂x} = \\dfrac{∂z}{∂p} \\dfrac{∂p}{∂x} + \\dfrac{∂z}{∂q} \\dfrac{∂q}{∂x} = 1⋅y+1⋅0 = y\n",
    "```\n",
    "\n",
    "For ∂z/∂y:\n",
    "```math\n",
    "\\dfrac{∂z}{∂y} = \\dfrac{∂z}{∂p} \\dfrac{∂p}{∂y} + \\dfrac{∂z}{∂q} \\dfrac{∂q}{∂y} = 1⋅x+1⋅2y = x+2y\n",
    "```\n",
    "\n",
    "These equations correspond to the chain rule calculations happening behind the scenes, demonstrating how PyTorch's autograd system computes gradients through the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888a04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x: tensor([3., 7.])\n",
      "Gradient of y: tensor([ 8., 19.])\n",
      "Gradient of z: tensor([1., 1.])\n",
      "Result of the operation: z = tensor([15., 84.])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with requires_grad=True\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([3.0, 7.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "z = x * y + y**2\n",
    "\n",
    "z.retain_grad() #By default intermediate layer weight updation is not shown.\n",
    "\n",
    "# Compute the gradients\n",
    "z_sum = z.sum().backward()\n",
    "\n",
    "\n",
    "print(f\"Gradient of x: {x.grad}\")\n",
    "print(f\"Gradient of y: {y.grad}\")\n",
    "print(f\"Gradient of z: {z.grad}\")\n",
    "print(f\"Result of the operation: z = {z.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84e1a9",
   "metadata": {},
   "source": [
    "### 1.2. Gradient Computation Graph\n",
    "\n",
    "A computation graph is a visual representation of the sequence of operations performed on tensors in a neural network, showing how each operation contributes to the final result. It is crucial for understanding and debugging the flow of data and gradients in deep learning models.\n",
    "\n",
    "torchviz is a tool used to visualize the computation graph of any PyTorch model.\n",
    "\n",
    "![Operations Graph](media/Autograd-Operators-Graph-1-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbf942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grad_computation_graph.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Visualize the computation graph\n",
    "dot = make_dot(z, params={\"x\": x, \"y\": y, \"z\" : z})\n",
    "dot.render(\"grad_computation_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa32c90",
   "metadata": {},
   "source": [
    "### 1.3. Detaching Tensors from Computation Graph\n",
    "\n",
    "The detach() method is used to create a new tensor that shares storage with the original tensor but without tracking operations. When you call detach(), it returns a new tensor that does not require gradients. This is useful when you want to perform operations on a tensor without affecting the computation graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before detaching z from computation:  True\n",
      "After detaching z from computation:  False\n"
     ]
    }
   ],
   "source": [
    "# Let's detach z from the computation graph\n",
    "print(\"Before detaching z from computation: \", z.requires_grad)\n",
    "z_det = z.detach()\n",
    "print(\"After detaching z from computation: \", z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669fc6f",
   "metadata": {},
   "source": [
    "### 1.4. Can Backpropagation be performed when requires_grad=False?\n",
    "\n",
    "Now the same tensors x and y are created with requires_grad=False.\n",
    "\n",
    "When attempting to compute the gradients using z.backward(), a RuntimeError is raised because the tensors do not require gradients, and thus do not have a grad_fn.\n",
    "\n",
    "In this case, since requires_grad=False was used, the computation graph is essentially empty, as no gradients will be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a5f841",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m z = x * y + y**\u001b[32m2\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Compute the gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "y = torch.tensor(3.0, requires_grad=False)\n",
    "\n",
    "\n",
    "# Perform simple operations\n",
    "z = x * y + y**2\n",
    "\n",
    "\n",
    "# Compute the gradients\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f94d9a",
   "metadata": {},
   "source": [
    "# 2. Backpropagation in Neural Networks\n",
    "\n",
    "![Gradient Descent](media/gradient-descent-2d-diagram.png)\n",
    "\n",
    "We have understood the mathematical intuition behind and how torch.autograd takes care of automatic differentiation with an example.\n",
    "\n",
    "Then,\n",
    "\n",
    "- The loss is calculate between prediction and target using loss(predcition,target)\n",
    "- Then backpropagation is performed using loss.backward()\n",
    "- We update the new weights using optimizer.step()\n",
    "\n",
    "![keras-linear-regression-weight-update-block-diagram](media/keras-linear-regression-weight-update-block-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207a0cc",
   "metadata": {},
   "source": [
    "The Weight Update Formula is:\n",
    "\n",
    "```math\n",
    "w_{new} = w - \\eta \\nabla L(w)\n",
    "```\n",
    "\n",
    "The gradient of L is a vector of partial derivatives:\n",
    "\n",
    "```math\n",
    "\\nabla L = \\left(\\dfrac{∂L}{∂w_1}, \\dfrac{∂L}{∂w_2}, ... , \\dfrac{∂L}{∂w_n}\\right)\n",
    "```\n",
    "\n",
    "- Current Weights (w): These are the weights of the model before the update.\n",
    "- Learning Rate (η): A hyperparameter that controls the step size of the weight update. A smaller learning rate makes the training process slower but more precise, while a larger learning rate makes the training process faster but it may sometimes overshoot the optimal solution.\n",
    "- Gradient of the Loss (∇L(w)): The partial derivatives of the loss function with respect to each weight. This indicates the direction and magnitude of the steepest ascent in the loss function.\n",
    "- Updated Weights (wnew): The new weights after applying the gradient descent step.\n",
    "\n",
    "After applying the updates, it's crucial to zero out the gradients. This is typically done using: optimizer.zer_grad()\n",
    "\n",
    "Interesting, right? This is how the model learns. During training, the model iteratively adjusts its weights and biases to minimize the loss function, which measures the difference between the predicted outputs and the actual targets.\n",
    "\n",
    "This process of forward pass, gradient computation, backward pass, and parameter update is repeated for many iterations (epochs) over the entire dataset. With each iteration, the model's parameters are refined, gradually reducing the loss and improving the model's performance.\n",
    "\n",
    "In our upcoming notebooks we will understand all these concepts with hands on training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e64ee",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Conclusion\n",
    "\n",
    "We learnt how the gradients are updated mathematically with a node graph diagrams. We also visualized the computation graph using torchviz, providing a clear understanding of how operations are structured in PyTorch. Finally, we understood how weight updates happen during training with code, illustrating the core mechanism of model learning.\n",
    "\n",
    "While this notebook provides a great start, our full course offers detailed explanations, hands-on projects, and expert guidance to help you master PyTorch and deep learning. Join here: [Deep Learning with PyTorch Course](https://opencv.org/university/deep-learning-with-pytorch/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
